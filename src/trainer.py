"""
è®­ç»ƒå™¨æ¨¡å—
åŒ…å«å®Œæ•´çš„è®­ç»ƒã€éªŒè¯ã€checkpointç®¡ç†é€»è¾‘
"""

import os
import torch
import logging
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR
from tqdm import tqdm
from .utils import AverageMeter, visualize_mesh_comparison, visualize_mesh_multichannel


class Trainer:
    """
    FNO è®­ç»ƒå™¨ç±»
    
    è´Ÿè´£æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€checkpoint ä¿å­˜ä¸åŠ è½½ã€TensorBoard æ—¥å¿—è®°å½•ç­‰ã€‚
    """
    def __init__(
        self, 
        model, 
        optimizer, 
        scheduler,
        loss_fn, 
        train_loader, 
        val_loader, 
        device, 
        config, 
        exp_dir
    ):
        """
        Args:
            model (nn.Module): æ¨¡å‹
            optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
            loss_fn (callable): æŸå¤±å‡½æ•°
            train_loader (DataLoader): è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader (DataLoader): éªŒè¯æ•°æ®åŠ è½½å™¨
            device (torch.device): è®¾å¤‡
            config (dict): é…ç½®å­—å…¸
            exp_dir (str): å®éªŒç›®å½•è·¯å¾„
        """
        self.model = model
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.loss_fn = loss_fn
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.config = config
        self.exp_dir = exp_dir
        
        # æå–å¸¸ç”¨é…ç½®
        self.num_epochs = config['train_params']['num_epochs']
        self.save_freq = config['log_params']['save_freq']
        self.val_freq = config['log_params']['val_freq']
        self.gradient_clip = config['train_params'].get('gradient_clip', None)
        
        # checkpoint ç›®å½•
        self.checkpoint_dir = os.path.join(exp_dir, 'checkpoints')
        
        # åˆå§‹åŒ– TensorBoard
        self._setup_logging()
        
        # è®°å½•æœ€ä½³æ€§èƒ½
        self.best_val_loss = float('inf')
        self.start_epoch = 0
        
        # æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger(__name__)
    
    def _setup_logging(self):
        """åˆå§‹åŒ– TensorBoard SummaryWriter"""
        tensorboard_dir = os.path.join(self.exp_dir, 'tensorboard')
        self.writer = SummaryWriter(tensorboard_dir)
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"TensorBoard æ—¥å¿—ä¿å­˜è‡³: {tensorboard_dir}")
    
    def _prep_batch(self, inputs, targets):
        """
        - æ”¯æŒ (B,C,H,W) -> (B*C,1,H,W) å±•å¼€
        - æ”¯æŒ 'offset' è®­ç»ƒï¼šç›®æ ‡ = GT - base_grid
        - è¿”å›: inputs_bhw1 (B*,H,W,1), targets_b2hw (B*,2,H,W), base_grid (B*,2,H,W)
        """
        # å±•å¼€å¤šé€šé“
        if inputs.dim() == 4 and inputs.size(1) > 1:
            B, C, H, W = inputs.shape
            inputs = inputs.view(B * C, 1, H, W)
            targets = targets.view(B, 2 * C, H, W).view(B * C, 2, H, W)

        # è®¾å¤‡
        inputs = inputs.to(self.device)
        targets = targets.to(self.device)

        # æ ‡å‡†ç½‘æ ¼ (ä¸æ¨¡å‹ get_grid çš„ [0,1] ä¸€è‡´)
        with torch.no_grad():
            H, W = inputs.size(2), inputs.size(3)
            # gridx = torch.linspace(0, 1, H, device=inputs.device).view(1, -1, 1).expand(inputs.size(0), -1, W)
            # gridy = torch.linspace(0, 1, W, device=inputs.device).view(1, 1, -1).expand(inputs.size(0), H, -1)
            # base_grid = torch.stack([gridx, gridy], dim=1)  # (B*,2,H,W)
            gridx = torch.linspace(0, 1, W, device=inputs.device).view(1, 1, -1).expand(inputs.size(0), H, -1)
            gridy = torch.linspace(1, 0, H, device=inputs.device).view(1, -1, 1).expand(inputs.size(0), -1, W)
            base_grid = torch.stack([gridx, gridy], dim=1)  # shape: (B,2,H,W)

        predict_mode = self.config['train_params'].get('predict_mode', 'offset')
        if predict_mode == 'offset':
            targets = targets - base_grid
        elif predict_mode == 'absolute':
            pass
        else:
            raise ValueError(f"Unknown predict_mode: {predict_mode}")

        # FNO è¾“å…¥éœ€è¦ (B*,H,W,1)
        inputs = inputs.permute(0, 2, 3, 1)

        return inputs, targets, base_grid
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹è®­ç»ƒ")
        self.logger.info("=" * 60)
        
        # å¦‚æœéœ€è¦æ–­ç‚¹ç»­è®­
        if self.config['checkpoint_params']['resume_from_checkpoint']:
            self._load_checkpoint()
        
        for epoch in range(self.start_epoch, self.num_epochs):
            self.logger.info(f"\nEpoch {epoch+1}/{self.num_epochs}")
            self.logger.info("-" * 60)
            
            # è®­ç»ƒä¸€ä¸ª epoch
            train_loss = self._train_epoch(epoch)
            
            # è®°å½•å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            self.writer.add_scalar('Learning_Rate', current_lr, epoch)
            self.logger.info(f"å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # éªŒè¯
            if (epoch + 1) % self.val_freq == 0 or epoch == self.num_epochs - 1:
                val_loss = self._validate_epoch(epoch)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä¼˜æ¨¡å‹
                is_best = val_loss < self.best_val_loss
                if is_best:
                    self.best_val_loss = val_loss
                    self.logger.info(f"ğŸ‰ å‘ç°æ›´ä¼˜æ¨¡å‹! éªŒè¯æŸå¤±: {val_loss:.6f}")
            else:
                is_best = False
            
            # ä¿å­˜ checkpoint
            if (epoch + 1) % self.save_freq == 0 or epoch == self.num_epochs - 1:
                self._save_checkpoint(epoch, is_best)
            
            # æ›´æ–°å­¦ä¹ ç‡ï¼ˆOneCycleLR åœ¨è®­ç»ƒå¾ªç¯ä¸­æ¯ä¸ªbatchæ›´æ–°ï¼Œè¿™é‡Œè·³è¿‡ï¼‰
            if self.scheduler is not None and not isinstance(self.scheduler, OneCycleLR):
                if isinstance(self.scheduler, ReduceLROnPlateau):
                    # ReduceLROnPlateau éœ€è¦éªŒè¯æŸå¤±
                    self.scheduler.step(val_loss if 'val_loss' in locals() else train_loss)
                else:
                    # å…¶ä»–è°ƒåº¦å™¨æŒ‰epochæ›´æ–°
                    self.scheduler.step()
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("è®­ç»ƒå®Œæˆ!")
        self.logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
        self.logger.info("=" * 60)
        self.writer.close()
    
    def _train_epoch(self, epoch):
        """
        è®­ç»ƒä¸€ä¸ª epoch
        
        Args:
            epoch (int): å½“å‰ epoch ç¼–å·
            
        Returns:
            float: è¯¥ epoch çš„å¹³å‡æŸå¤±
        """
        self.model.train()
        loss_meter = AverageMeter()
        # ä¸ºå„é¡¹æŸå¤±åˆ›å»ºmeter
        loss_components_meters = {}
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(self.train_loader, desc=f"è®­ç»ƒ Epoch {epoch+1}")
        
        for batch_idx, (inputs, targets) in enumerate(pbar):
            inputs, targets, base_grid = self._prep_batch(inputs, targets)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(inputs)  # (B, 2, H, W)
            
            # è®¡ç®—æŸå¤±ï¼ˆä¼ å…¥base_gridç”¨äºBeltramiçº¦æŸï¼‰
            loss, loss_dict = self.loss_fn(outputs, targets, base_grid=base_grid)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.gradient_clip is not None:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
            
            self.optimizer.step()
            
            # OneCycleLR åœ¨æ¯ä¸ªbatchåæ›´æ–°å­¦ä¹ ç‡
            if self.scheduler is not None and isinstance(self.scheduler, OneCycleLR):
                self.scheduler.step()
            
            # æ›´æ–°æ€»æŸå¤±ç»Ÿè®¡
            loss_meter.update(loss.item(), inputs.size(0))
            
            # æ›´æ–°å„é¡¹æŸå¤±ç»Ÿè®¡
            for key, value in loss_dict.items():
                if key not in loss_components_meters:
                    loss_components_meters[key] = AverageMeter()
                loss_components_meters[key].update(value, inputs.size(0))
            
            # æ›´æ–°è¿›åº¦æ¡ï¼ˆåªæ˜¾ç¤ºæ€»æŸå¤±ï¼‰
            pbar.set_postfix({'loss': f'{loss_meter.avg:.6f}'})
            
            # è®°å½•åˆ° TensorBoard (æ¯ä¸ªbatch)
            global_step = epoch * len(self.train_loader) + batch_idx
            self.writer.add_scalar('Train/Loss_Step', loss.item(), global_step)
        
        # è®°å½• epoch çº§åˆ«çš„æŸå¤±
        self.writer.add_scalar('Train/Loss_Epoch', loss_meter.avg, epoch)
        
        # æ‰“å°æ€»æŸå¤±å’Œå„é¡¹æŸå¤±
        loss_info = f"è®­ç»ƒæŸå¤±: {loss_meter.avg:.6f}"
        if loss_components_meters:
            # æ„å»ºåˆ†é¡¹æŸå¤±å­—ç¬¦ä¸²
            components_str = " | ".join([
                f"{key}: {meter.avg:.6f}" 
                for key, meter in loss_components_meters.items() 
                if key != 'total'  # é¿å…é‡å¤æ˜¾ç¤ºtotal
            ])
            loss_info += f" ({components_str})"
        
        self.logger.info(loss_info)
        
        return loss_meter.avg
    
    def _validate_epoch(self, epoch):
        """
        éªŒè¯ä¸€ä¸ª epoch
        
        Args:
            epoch (int): å½“å‰ epoch ç¼–å·
            
        Returns:
            float: éªŒè¯é›†çš„å¹³å‡æŸå¤±
        """
        self.model.eval()
        loss_meter = AverageMeter()
        # ä¸ºå„é¡¹æŸå¤±åˆ›å»ºmeter
        loss_components_meters = {}
        
        # ç”¨äºå­˜å‚¨ç¬¬ä¸€ä¸ªbatchçš„å¯è§†åŒ–æ•°æ®
        first_batch_raw_inputs = None
        first_batch_raw_targets = None
        first_batch_outputs = None
        first_batch_base_grid = None
        
        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc=f"éªŒè¯ Epoch {epoch+1}")
            
            for batch_idx, (raw_inputs, raw_targets) in enumerate(pbar):
                # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºå¯è§†åŒ–ï¼ˆç¬¬ä¸€ä¸ªbatchï¼‰
                if batch_idx == 0:
                    first_batch_raw_inputs = raw_inputs
                    first_batch_raw_targets = raw_targets
                
                inputs, targets, base_grid = self._prep_batch(raw_inputs, raw_targets)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(inputs)
                
                # è®¡ç®—æŸå¤±ï¼ˆä¼ å…¥base_gridç”¨äºBeltramiçº¦æŸï¼‰
                loss, loss_dict = self.loss_fn(outputs, targets, base_grid=base_grid)
                
                # æ›´æ–°æ€»æŸå¤±ç»Ÿè®¡
                loss_meter.update(loss.item(), inputs.size(0))
                
                # æ›´æ–°å„é¡¹æŸå¤±ç»Ÿè®¡
                for key, value in loss_dict.items():
                    if key not in loss_components_meters:
                        loss_components_meters[key] = AverageMeter()
                    loss_components_meters[key].update(value, inputs.size(0))
                
                # æ›´æ–°è¿›åº¦æ¡ï¼ˆåªæ˜¾ç¤ºæ€»æŸå¤±ï¼‰
                pbar.set_postfix({'val_loss': f'{loss_meter.avg:.6f}'})
                
                # ä¿å­˜ç¬¬ä¸€ä¸ªbatchçš„å¤„ç†åæ•°æ®ç”¨äºå¯è§†åŒ–
                if batch_idx == 0:
                    first_batch_outputs = outputs
                    first_batch_base_grid = base_grid
        
        # è®°å½•åˆ° TensorBoard
        self.writer.add_scalar('Val/Loss', loss_meter.avg, epoch)
        
        # æ‰“å°æ€»æŸå¤±å’Œå„é¡¹æŸå¤±
        loss_info = f"éªŒè¯æŸå¤±: {loss_meter.avg:.6f}"
        if loss_components_meters:
            # æ„å»ºåˆ†é¡¹æŸå¤±å­—ç¬¦ä¸²
            components_str = " | ".join([
                f"{key}: {meter.avg:.6f}" 
                for key, meter in loss_components_meters.items() 
                if key != 'total'  # é¿å…é‡å¤æ˜¾ç¤ºtotal
            ])
            loss_info += f" ({components_str})"
        
        self.logger.info(loss_info)
        
        # ç½‘æ ¼å¯è§†åŒ–
        vis_params = self.config.get('visualization_params', {})
        enable_mesh_vis = vis_params.get('enable_mesh_vis', True)
        
        if first_batch_outputs is not None and enable_mesh_vis:
            num_samples = vis_params.get('vis_num_samples', 1)
            self._visualize_meshes(
                first_batch_raw_inputs,
                first_batch_raw_targets,
                first_batch_outputs, 
                first_batch_base_grid,
                epoch,
                num_samples=num_samples
            )
        
        return loss_meter.avg
    
    def _visualize_meshes(self, raw_inputs, raw_targets, outputs, base_grid, epoch, num_samples=1):
        """
        å¯è§†åŒ–ç½‘æ ¼å¯¹æ¯”å¹¶ä¿å­˜åˆ°TensorBoard
        
        Args:
            raw_inputs (torch.Tensor): åŸå§‹è¾“å…¥ï¼Œshape (B, C, H, W)
            raw_targets (torch.Tensor): åŸå§‹ç›®æ ‡ï¼Œshape (B, C*2, H, W)
            outputs (torch.Tensor): æ¨¡å‹è¾“å‡ºï¼Œshape (B*C, 2, H, W)
            base_grid (torch.Tensor): åŸºç¡€ç½‘æ ¼ï¼Œshape (B*C, 2, H, W)
            epoch (int): å½“å‰epoch
            num_samples (int): è¦å¯è§†åŒ–çš„æ ·æœ¬æ•°é‡
        """
        predict_mode = self.config['train_params'].get('predict_mode', 'offset')
        
        # æ£€æµ‹æ˜¯å•é€šé“è¿˜æ˜¯å¤šé€šé“
        if raw_inputs.dim() == 4 and raw_inputs.size(1) > 1:
            # å¤šé€šé“æƒ…å†µ (B, C, H, W)
            B, C, H, W = raw_inputs.shape
            is_multichannel = True
            num_channels = C
        else:
            # å•é€šé“æƒ…å†µ (B, 1, H, W)
            is_multichannel = False
            num_channels = 1
        
        # å°†é¢„æµ‹è½¬æ¢ä¸ºabsoluteåæ ‡
        if predict_mode == 'offset':
            pred_absolute = outputs + base_grid
        else:
            pred_absolute = outputs
        
        # å¤„ç†ç›®æ ‡åæ ‡ï¼ˆraw_targets å·²ç»æ˜¯ç»å¯¹åæ ‡ï¼‰
        target_absolute = raw_targets.to(self.device)
        if is_multichannel:
            # å¤šé€šé“: (B, C*2, H, W) -> (B*C, 2, H, W)
            B, C2, H, W = target_absolute.shape
            target_absolute = target_absolute.view(B, C2 // 2, 2, H, W).reshape(B * (C2 // 2), 2, H, W)
        
        # å¯è§†åŒ–ç¬¬ä¸€ä¸ªæ ·æœ¬
        sample_idx = 0
        
        if is_multichannel and num_channels > 1:
            # å¤šé€šé“æƒ…å†µï¼šä¸ºæ¯ä¸ªé€šé“ç”Ÿæˆä¸€å¼ å›¾
            self.logger.info(f"å¯è§†åŒ–å¤šé€šé“ç½‘æ ¼ ({num_channels} é€šé“)")
            
            for c in range(num_channels):
                idx = sample_idx * num_channels + c
                if idx >= pred_absolute.size(0):
                    break
                
                pred_mesh = pred_absolute[idx]  # (2, H, W)
                gt_mesh = target_absolute[idx]  # (2, H, W)
                
                try:
                    # ç”Ÿæˆå¯è§†åŒ–å›¾åƒ
                    img_array = visualize_mesh_comparison(
                        gt_mesh, 
                        pred_mesh, 
                        fig_size=(8, 8), 
                        dpi=100
                    )
                    
                    # è½¬æ¢ä¸ºTensorBoardæ ¼å¼ (C, H, W)ï¼ŒèŒƒå›´[0, 1]
                    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
                    
                    # ä¿å­˜åˆ°TensorBoard
                    self.writer.add_image(
                        f'Val/Mesh_Comparison_Channel_{c}', 
                        img_tensor, 
                        epoch
                    )
                    
                    self.logger.info(f"å·²ä¿å­˜ç½‘æ ¼å¯è§†åŒ– (é€šé“ {c}) åˆ° TensorBoard")
                    
                except Exception as e:
                    self.logger.warning(f"ç½‘æ ¼å¯è§†åŒ–å¤±è´¥ (é€šé“ {c}): {str(e)}")
        else:
            # å•é€šé“æƒ…å†µ
            self.logger.info("å¯è§†åŒ–å•é€šé“ç½‘æ ¼")
            
            for idx in range(min(num_samples, pred_absolute.size(0))):
                pred_mesh = pred_absolute[idx]  # (2, H, W)
                gt_mesh = target_absolute[idx]  # (2, H, W)
                
                try:
                    # ç”Ÿæˆå¯è§†åŒ–å›¾åƒ
                    img_array = visualize_mesh_comparison(
                        gt_mesh, 
                        pred_mesh, 
                        fig_size=(8, 8), 
                        dpi=100
                    )
                    
                    # è½¬æ¢ä¸ºTensorBoardæ ¼å¼ (C, H, W)ï¼ŒèŒƒå›´[0, 1]
                    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
                    
                    # ä¿å­˜åˆ°TensorBoard
                    self.writer.add_image(
                        f'Val/Mesh_Comparison_Sample_{idx}', 
                        img_tensor, 
                        epoch
                    )
                    
                    self.logger.info(f"å·²ä¿å­˜ç½‘æ ¼å¯è§†åŒ– (æ ·æœ¬ {idx}) åˆ° TensorBoard")
                    
                except Exception as e:
                    self.logger.warning(f"ç½‘æ ¼å¯è§†åŒ–å¤±è´¥ (æ ·æœ¬ {idx}): {str(e)}")
    
    def _save_checkpoint(self, epoch, is_best=False):
        """
        ä¿å­˜ checkpoint
        
        Args:
            epoch (int): å½“å‰ epoch
            is_best (bool): æ˜¯å¦ä¸ºæœ€ä¼˜æ¨¡å‹
        """
        checkpoint = {
            'epoch': epoch + 1,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # ä¿å­˜æœ€æ–°çš„ checkpoint
        last_path = os.path.join(self.checkpoint_dir, 'last.pth')
        torch.save(checkpoint, last_path)
        self.logger.info(f"å·²ä¿å­˜ checkpoint: {last_path}")
        
        # å¦‚æœæ˜¯æœ€ä¼˜æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜ä¸€ä»½
        if is_best and self.config['checkpoint_params']['save_best']:
            best_path = os.path.join(self.checkpoint_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
            self.logger.info(f"å·²ä¿å­˜æœ€ä¼˜æ¨¡å‹: {best_path}")
    
    def _load_checkpoint(self):
        """ä» checkpoint æ¢å¤è®­ç»ƒ"""
        checkpoint_path = self.config['checkpoint_params']['resume_from_checkpoint']
        
        if not os.path.exists(checkpoint_path):
            self.logger.warning(f"Checkpoint è·¯å¾„ä¸å­˜åœ¨: {checkpoint_path}")
            return
        
        self.logger.info(f"ä» checkpoint æ¢å¤: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # åŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
        if self.scheduler is not None and 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # æ¢å¤è®­ç»ƒè¿›åº¦
        self.start_epoch = checkpoint['epoch']
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        self.logger.info(f"å·²æ¢å¤åˆ° Epoch {self.start_epoch}")
        self.logger.info(f"å½“å‰æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")

